save(df,file=name)
# Clean up model
deleteGPsep(gp_model)
df
plot_ps_posterior_regr(grid=Xtest,posterior,mXtest,data)
##########################################################
######## FUNCTIONS KERNEL  REGRESSION PredResampl ########
#########################################################
initialization_kernel_regr_resampling <- function(data,M,bw="pi"){
'Determines a sequence of bandwidth
- data: the first column is y,then the covariates. It is n x (p+1)
- M: desidered number of synthetic samples'
'The function goes as follows:
1: bandwdith from 1 to n are chosen to be equal to the value given by the chosen criteria:
- scv: SCV selector of Jones Marron Park (1991)
- pi : plug in method of Wand and Jones (1994)
2- 5 proceeds as the univariate function'
n <- dim(data)[1]
bw_function <- match.fun(paste0("H", bw,".diag"))
#1.
b1 <- diag(bw_function(data))
#2 & 3
b2 <- c()
for (i in 1:1){
dataB <- data[sample(n,M,replace=T),]
b2 <- cbind(b2,diag(bw_function(rbind(data,dataB))))
}
b2 <- rowMeans(b2)
#4
a1 <- b1
a2 <- log(b1/b2)/M
#5
h <- bandwidth_matrix(a1,a2,M)
return(h)
}
bandwidth_matrix <- function(a1,a2,M){
"Generate a bandwidth sequence in the multivariate case"
h0 <- matrix(rep(a1,n),nrow=n,byrow=T)
h1 <- exp(-outer(seq(1,M),a2))* matrix(a1,nrow=M,ncol=dim(h0)[2],byrow=TRUE)
return(rbind(h0,h1))
}
ps_posterior_regr <- function(data,grid,h,M=1000,B=100,kernel="gaussian",algo="recursive",bw="ndr"){
"Implement Predictive resampling posterior for a regression function
Inputs:
- algo: recursive it should use the recursive kernel estimator, standard if uses the standard kernel density estimatr
Returns:
- mean
- median
- 97.5% and 2.5% bands"
#it is still 1d
posterior <- matrix(0,ncol=nrow(grid),nrow=B)
if (algo=="recursive") {
for (b in 1:B){
posterior[b,] <- recursive_kernel_regr(data,grid,h,kernel,ps=TRUE,M)
}
} else if (algo=="standard"){
for (b in 1:B){
posterior[b,] <- standard_kernel(y,grid,h,kernel,ps=TRUE,M,bw)
}
} else {
print("which estimator you want to use?")
}
q2.5 <- apply(data.frame(posterior), 2, quantile, probs=0.025)
q97.5 <- apply(data.frame(posterior), 2, quantile, probs=0.975)
q50 <- apply(data.frame(posterior), 2, quantile, probs=0.5)
mean <- apply(data.frame(posterior), 2, mean)
posterior <- list(grid=grid,mean=mean,q2.5=q2.5,q97.5=q97.5,q50=q50)
return(posterior)
}
recursive_kernel_regr <- function(data,grid,h,kernel="gaussian",ps=TRUE,M=1000){
'Implement the kernel recursive regression estimator,
it can generate predictive resampling samples!'
n <- dim(data)[1]
d <- dim(data)[2]
if (ps==TRUE){
dataB <- ps_kernel_mv(data,M,h,kernel)
data  <- rbind(data,dataB)
}
y <- data[,1]
X <- matrix(data[,2:d],ncol=(d-1)) #Check when it is multivariate!
m <- kernel_regression(X, y, h, grid,kernel)
return(m)
}
ps_kernel_mv <- function(data,M,h,kernel="gaussian"){
'Implement a predictive resampling with kernels multivariate (single run!)
- data: data
- M: # synthetic data
- kernel type'
n <- dim(data)[1]
d <- dim(data)[2]
Y <- data
if (kernel=="gaussian"){
for (i in 1:M){
j <- sample(n+i-1,1)
Y <- rbind(Y,rmvnorm(1,mean=Y[j,], sigma= diag(h[j,])))
}
} else if (kernel=="uniform"){
for (i in 1:M){
j <- sample(n+i-1,1)
Y <- rbind(Y,runif(d,min=Y[j,]-h[j,],max=Y[j,]+h[j,]))
}
} else if (kernel=="laplace"){
for (i in 1:M){
j <- sample(n+i-1,1)
Y <- rbind(Y,mapply(rlaplace, n = 1, location=Y[j,],scale=h[j,]))
}
} else {
print("Kernel not yet implemented!")
}
boot <- Y[n+1:M,]
return(boot)
}
gaussian_kernel_regression <- function(X, y, h, grid) {
# X: matrix of predictors (n x p)
# y: vector of responses (n)
# h: matrix of bandwidth
# x_new: new data points to predict (m x p)
# Gaussian Kernel Function
kernel <- function(x, x_i, h) {
exp(-sum((x - x_i)^2 / (2 * h^2)))
}
# Predict for each grid_i (each x in which I am evaluating the grid)
m <- sapply(1:nrow(grid), function(i) {
grid_i <- grid[i, ]
# Compute kernel weights
weights <- sapply(1:nrow(X), function(j) kernel(grid_i, X[j, ], h[j,]))
# Weighted sum of responses
weighted_sum <- sum(weights * y) / sum(weights)
return(weighted_sum)
})
return(m)
}
kernel_regression <- function(X, y, h, grid, kernel="gaussian") {
# X: matrix of predictors (n x p)
# y: vector of responses (n)
# h: matrix of bandwidth
# x_new: new data points to predict (m x p)
if (kernel=="gaussian"){
kernel <- function(x, x_i, h) {
exp(-sum((x - x_i)^2 / (2 * h^2)))
}
} else if (kernel=="uniform"){
kernel <- function(x, x_i, h) {
all(x >= (x_i - h) & x <= (x_i + h))
}
} else if (kernel=="laplace"){
kernel <- function(x, x_i, h) {
exp(-sum(abs(x - x_i) / ( h)))
}
}
# Predict for each grid_i (each x in which I am evaluating the grid)
m <- sapply(1:nrow(grid), function(i) {
grid_i <- grid[i, ]
# Compute kernel weights
weights <- sapply(1:nrow(X), function(j) kernel(grid_i, X[j, ], h[j,]))
# Weighted sum of responses
weighted_sum <- sum(weights * y) / sum(weights)
return(weighted_sum)
})
return(m)
}
plot_ps_posterior_regr <- function(grid,posterior,mX,data,ylim=c(),xlim=c()){
y <- data[,1]
X0 <- data[,2:ncol(data)]
if (length(ylim)==0){
ylim = c(min(c(posterior$q2.5,mX)),max(c(posterior$q97.5,mX)))
}
if (length(xlim)==0){
xlim <- c(min(grid),max(grid))
}
plot(grid,posterior$mean,type="l",ylim=ylim,xlim=xlim,lwd=2)
polygon(c(grid, rev(grid)), c(posterior$q2.5, rev(posterior$q97.5)),
col = "#D3D3D3")
lines(grid,posterior$mean,lwd=2)
lines(grid,mX, lty=2,lwd=2)
lines(grid,posterior$q2.5,col="black")
lines(grid,posterior$q97.5,col="black")
points(X0,y)
}
plot_ps_posterior_regr(grid=Xtest,posterior,mXtest,data)
q2.5 <- apply(data.frame(reg_means), 2, quantile, probs=0.025)
q97.5 <- apply(data.frame(reg_means), 2, quantile, probs=0.975)
q50 <- apply(data.frame(reg_means), 2, quantile, probs=0.5)
mean <- apply(data.frame(reg_means), 2, mean)
posterior <- list(grid=Xtest,mean=mean,q2.5=q2.5,q97.5=q97.5,q50=q50)
plot_ps_posterior_regr(grid=Xtest,posterior,mXtest,data)
for (k in c("gaussian","laplace","uniform")){
for (d in c(0.5,0.7,0.9,1,1.1)){
for (b in c("pi","scv")){
h <- d*initialization_kernel_regr_resampling(data,M,bw="pi")
posterior <- ps_posterior_regr(data=data,grid=Xtest,h=h,M=M,B=B,kernel="gaussian")
method_name <- paste("kernel_",substr(k,1,1),"_",d,b,sep="")
df <- rbind(df,data.frame(n=n,data=idx,method=method_name,env=envelope(posterior,mXtest),
abs_bias=bias(posterior,mXtest)$abs,rel_bias=bias(posterior,mXtest)$rel,
abs_rwd=relwid(posterior,mXtest)$abs,rel_rwd=relwid(posterior,mXtest)$rel,
abs_dev=dev(posterior,mXtest)$abs,rel_dev=dev(posterior,mXtest)$rel,
MSE_med=mean((posterior$q50-ytest)^2),MSE_mean=mean((posterior$mean-ytest)^2)))
}}}
#Run
M <- 1000
B <- 400
for (k in c("gaussian","laplace","uniform")){
for (d in c(0.5,0.7,0.9,1,1.1)){
for (b in c("pi","scv")){
h <- d*initialization_kernel_regr_resampling(data,M,bw="pi")
posterior <- ps_posterior_regr(data=data,grid=Xtest,h=h,M=M,B=B,kernel="gaussian")
method_name <- paste("kernel_",substr(k,1,1),"_",d,b,sep="")
df <- rbind(df,data.frame(n=n,data=idx,method=method_name,env=envelope(posterior,mXtest),
abs_bias=bias(posterior,mXtest)$abs,rel_bias=bias(posterior,mXtest)$rel,
abs_rwd=relwid(posterior,mXtest)$abs,rel_rwd=relwid(posterior,mXtest)$rel,
abs_dev=dev(posterior,mXtest)$abs,rel_dev=dev(posterior,mXtest)$rel,
MSE_med=mean((posterior$q50-ytest)^2),MSE_mean=mean((posterior$mean-ytest)^2)))
}}}
h <- d*initialization_kernel_regr_resampling(data,M,bw="pi")
posterior <- ps_posterior_regr(data=data,grid=Xtest,h=h,M=M,B=B,kernel="gaussian")
plot_ps_posterior_regr(grid=Xtest,posterior,mXtest,data)
rm(list=ls())
library(plgp)
generate_regression_data <- function(n,type){
#X0 <- matrix(seq(0, 5, length=n), ncol=1)
Xtrain <- matrix(sort(5*runif(n)), ncol=1)
Xtest <-  matrix(seq(0,4.95,0.05),ncol=1)
X <- matrix(sort(rbind(Xtrain,Xtest)),ncol=1)
D <- distance(X)
eps <- sqrt(.Machine$double.eps)
if (type=="g"){
Sigma <- exp(-D) + diag(eps, nrow(X))
mX  <- rmvnorm(1, sigma=Sigma)
y <- matrix(mX + rnorm(nrow(X)),ncol=1)
} else if (type=="t"){
Sigma <- exp(-D) + diag(eps, nrow(X))
mX  <- rmvnorm(1, sigma=Sigma)
y <- matrix(mX + rt(nrow(X),df=5),ncol=1)
} else if (type=="k"){
D1 <- sqrt(D)/20
Sigma <- 0.8*exp(-D)+0.2*exp(-D1) + diag(eps, nrow(X))
mX  <- rmvnorm(1, sigma=Sigma)
y <- matrix(mX + rnorm(nrow(X)),ncol=1)
}
idtrain <- X %in% Xtrain
ytrain <- y[idtrain]
ytest <- y[!idtrain]
mXtest <- mX[!idtrain]
#plot(X, mX, type="l")
#points(X,y)
train <- data.frame(ytrain,Xtrain)
test <- data.frame(ytest,Xtest,mXtest)
return(list(train=train,test=test))
}
# Example usage:
set.seed(123)
n <- 100
type <- "g"
for (i in 1:100){
result <- generate_regression_data(n,type)
train <- result$train
test <- result$test
name <- paste("data_train/data_train_",type,"_n_",n,"_",i,".csv",sep="")
write.table(train, name, sep = ",", row.names = FALSE, col.names = FALSE)
name <- paste("data_test/data_test_",type,"_n_",n,"_",i,".csv",sep="")
write.table(test, name, sep = ",", row.names = FALSE, col.names = FALSE)
}
#hist(y, breaks = 50, col = "lightblue", main = "Generated Mixture Data", freq = FALSE)
#lines(grid, true, col = "red", lwd = 2)
lines(test$Xtest,test$mXtest,col="red")
rm(list=ls())
library(plgp)
generate_regression_data <- function(n,type){
#X0 <- matrix(seq(0, 5, length=n), ncol=1)
Xtrain <- matrix(sort(5*runif(n)), ncol=1)
Xtest <-  matrix(seq(0,4.95,0.05),ncol=1)
X <- matrix(sort(rbind(Xtrain,Xtest)),ncol=1)
D <- distance(X)
eps <- sqrt(.Machine$double.eps)
if (type=="g"){
Sigma <- exp(-D) + diag(eps, nrow(X))
mX  <- rmvnorm(1, sigma=Sigma)
y <- matrix(mX + rnorm(nrow(X)),ncol=1)
} else if (type=="t"){
Sigma <- exp(-D) + diag(eps, nrow(X))
mX  <- rmvnorm(1, sigma=Sigma)
y <- matrix(mX + rt(nrow(X),df=5),ncol=1)
} else if (type=="k"){
D1 <- sqrt(D)/20
Sigma <- 0.8*exp(-D)+0.2*exp(-D1) + diag(eps, nrow(X))
mX  <- rmvnorm(1, sigma=Sigma)
y <- matrix(mX + rnorm(nrow(X)),ncol=1)
}
idtrain <- X %in% Xtrain
ytrain <- y[idtrain]
ytest <- y[!idtrain]
mXtest <- mX[!idtrain]
#plot(X, mX, type="l")
#points(X,y)
train <- data.frame(ytrain,Xtrain)
test <- data.frame(ytest,Xtest,mXtest)
return(list(train=train,test=test))
}
# Example usage:
set.seed(123)
n <- 100
type <- "t"
for (i in 1:100){
result <- generate_regression_data(n,type)
train <- result$train
test <- result$test
name <- paste("data_train/data_train_",type,"_n_",n,"_",i,".csv",sep="")
write.table(train, name, sep = ",", row.names = FALSE, col.names = FALSE)
name <- paste("data_test/data_test_",type,"_n_",n,"_",i,".csv",sep="")
write.table(test, name, sep = ",", row.names = FALSE, col.names = FALSE)
}
#hist(y, breaks = 50, col = "lightblue", main = "Generated Mixture Data", freq = FALSE)
#lines(grid, true, col = "red", lwd = 2)
lines(test$Xtest,test$mXtest,col="red")
rm(list=ls())
library(plgp)
generate_regression_data <- function(n,type){
#X0 <- matrix(seq(0, 5, length=n), ncol=1)
Xtrain <- matrix(sort(5*runif(n)), ncol=1)
Xtest <-  matrix(seq(0,4.95,0.05),ncol=1)
X <- matrix(sort(rbind(Xtrain,Xtest)),ncol=1)
D <- distance(X)
eps <- sqrt(.Machine$double.eps)
if (type=="g"){
Sigma <- exp(-D) + diag(eps, nrow(X))
mX  <- rmvnorm(1, sigma=Sigma)
y <- matrix(mX + rnorm(nrow(X)),ncol=1)
} else if (type=="t"){
Sigma <- exp(-D) + diag(eps, nrow(X))
mX  <- rmvnorm(1, sigma=Sigma)
y <- matrix(mX + rt(nrow(X),df=5),ncol=1)
} else if (type=="k"){
D1 <- sqrt(D)/20
Sigma <- 0.8*exp(-D)+0.2*exp(-D1) + diag(eps, nrow(X))
mX  <- rmvnorm(1, sigma=Sigma)
y <- matrix(mX + rnorm(nrow(X)),ncol=1)
}
idtrain <- X %in% Xtrain
ytrain <- y[idtrain]
ytest <- y[!idtrain]
mXtest <- mX[!idtrain]
#plot(X, mX, type="l")
#points(X,y)
train <- data.frame(ytrain,Xtrain)
test <- data.frame(ytest,Xtest,mXtest)
return(list(train=train,test=test))
}
# Example usage:
set.seed(123)
n <- 100
type <- "k"
for (i in 1:100){
result <- generate_regression_data(n,type)
train <- result$train
test <- result$test
name <- paste("data_train/data_train_",type,"_n_",n,"_",i,".csv",sep="")
write.table(train, name, sep = ",", row.names = FALSE, col.names = FALSE)
name <- paste("data_test/data_test_",type,"_n_",n,"_",i,".csv",sep="")
write.table(test, name, sep = ",", row.names = FALSE, col.names = FALSE)
}
#hist(y, breaks = 50, col = "lightblue", main = "Generated Mixture Data", freq = FALSE)
#lines(grid, true, col = "red", lwd = 2)
lines(test$Xtest,test$mXtest,col="red")
rm(list=ls())
library(plgp)
generate_regression_data <- function(n,type){
#X0 <- matrix(seq(0, 5, length=n), ncol=1)
Xtrain <- matrix(sort(5*runif(n)), ncol=1)
Xtest <-  matrix(seq(0,4.95,0.05),ncol=1)
X <- matrix(sort(rbind(Xtrain,Xtest)),ncol=1)
D <- distance(X)
eps <- sqrt(.Machine$double.eps)
if (type=="g"){
Sigma <- exp(-D) + diag(eps, nrow(X))
mX  <- rmvnorm(1, sigma=Sigma)
y <- matrix(mX + rnorm(nrow(X)),ncol=1)
} else if (type=="t"){
Sigma <- exp(-D) + diag(eps, nrow(X))
mX  <- rmvnorm(1, sigma=Sigma)
y <- matrix(mX + rt(nrow(X),df=5),ncol=1)
} else if (type=="k"){
D1 <- sqrt(D)/20
Sigma <- 0.8*exp(-D)+0.2*exp(-D1) + diag(eps, nrow(X))
mX  <- rmvnorm(1, sigma=Sigma)
y <- matrix(mX + rnorm(nrow(X)),ncol=1)
}
idtrain <- X %in% Xtrain
ytrain <- y[idtrain]
ytest <- y[!idtrain]
mXtest <- mX[!idtrain]
#plot(X, mX, type="l")
#points(X,y)
train <- data.frame(ytrain,Xtrain)
test <- data.frame(ytest,Xtest,mXtest)
return(list(train=train,test=test))
}
# Example usage:
set.seed(123)
n <- 200
type <- "k"
for (i in 1:100){
result <- generate_regression_data(n,type)
train <- result$train
test <- result$test
name <- paste("data_train/data_train_",type,"_n_",n,"_",i,".csv",sep="")
write.table(train, name, sep = ",", row.names = FALSE, col.names = FALSE)
name <- paste("data_test/data_test_",type,"_n_",n,"_",i,".csv",sep="")
write.table(test, name, sep = ",", row.names = FALSE, col.names = FALSE)
}
#hist(y, breaks = 50, col = "lightblue", main = "Generated Mixture Data", freq = FALSE)
#lines(grid, true, col = "red", lwd = 2)
lines(test$Xtest,test$mXtest,col="red")
rm(list=ls())
library(plgp)
generate_regression_data <- function(n,type){
#X0 <- matrix(seq(0, 5, length=n), ncol=1)
Xtrain <- matrix(sort(5*runif(n)), ncol=1)
Xtest <-  matrix(seq(0,4.95,0.05),ncol=1)
X <- matrix(sort(rbind(Xtrain,Xtest)),ncol=1)
D <- distance(X)
eps <- sqrt(.Machine$double.eps)
if (type=="g"){
Sigma <- exp(-D) + diag(eps, nrow(X))
mX  <- rmvnorm(1, sigma=Sigma)
y <- matrix(mX + rnorm(nrow(X)),ncol=1)
} else if (type=="t"){
Sigma <- exp(-D) + diag(eps, nrow(X))
mX  <- rmvnorm(1, sigma=Sigma)
y <- matrix(mX + rt(nrow(X),df=5),ncol=1)
} else if (type=="k"){
D1 <- sqrt(D)/20
Sigma <- 0.8*exp(-D)+0.2*exp(-D1) + diag(eps, nrow(X))
mX  <- rmvnorm(1, sigma=Sigma)
y <- matrix(mX + rnorm(nrow(X)),ncol=1)
}
idtrain <- X %in% Xtrain
ytrain <- y[idtrain]
ytest <- y[!idtrain]
mXtest <- mX[!idtrain]
#plot(X, mX, type="l")
#points(X,y)
train <- data.frame(ytrain,Xtrain)
test <- data.frame(ytest,Xtest,mXtest)
return(list(train=train,test=test))
}
# Example usage:
set.seed(123)
n <- 200
type <- "t"
for (i in 1:100){
result <- generate_regression_data(n,type)
train <- result$train
test <- result$test
name <- paste("data_train/data_train_",type,"_n_",n,"_",i,".csv",sep="")
write.table(train, name, sep = ",", row.names = FALSE, col.names = FALSE)
name <- paste("data_test/data_test_",type,"_n_",n,"_",i,".csv",sep="")
write.table(test, name, sep = ",", row.names = FALSE, col.names = FALSE)
}
#hist(y, breaks = 50, col = "lightblue", main = "Generated Mixture Data", freq = FALSE)
#lines(grid, true, col = "red", lwd = 2)
lines(test$Xtest,test$mXtest,col="red")
rm(list=ls())
library(plgp)
generate_regression_data <- function(n,type){
#X0 <- matrix(seq(0, 5, length=n), ncol=1)
Xtrain <- matrix(sort(5*runif(n)), ncol=1)
Xtest <-  matrix(seq(0,4.95,0.05),ncol=1)
X <- matrix(sort(rbind(Xtrain,Xtest)),ncol=1)
D <- distance(X)
eps <- sqrt(.Machine$double.eps)
if (type=="g"){
Sigma <- exp(-D) + diag(eps, nrow(X))
mX  <- rmvnorm(1, sigma=Sigma)
y <- matrix(mX + rnorm(nrow(X)),ncol=1)
} else if (type=="t"){
Sigma <- exp(-D) + diag(eps, nrow(X))
mX  <- rmvnorm(1, sigma=Sigma)
y <- matrix(mX + rt(nrow(X),df=5),ncol=1)
} else if (type=="k"){
D1 <- sqrt(D)/20
Sigma <- 0.8*exp(-D)+0.2*exp(-D1) + diag(eps, nrow(X))
mX  <- rmvnorm(1, sigma=Sigma)
y <- matrix(mX + rnorm(nrow(X)),ncol=1)
}
idtrain <- X %in% Xtrain
ytrain <- y[idtrain]
ytest <- y[!idtrain]
mXtest <- mX[!idtrain]
#plot(X, mX, type="l")
#points(X,y)
train <- data.frame(ytrain,Xtrain)
test <- data.frame(ytest,Xtest,mXtest)
return(list(train=train,test=test))
}
# Example usage:
set.seed(123)
n <- 200
type <- "g"
for (i in 1:100){
result <- generate_regression_data(n,type)
train <- result$train
test <- result$test
name <- paste("data_train/data_train_",type,"_n_",n,"_",i,".csv",sep="")
write.table(train, name, sep = ",", row.names = FALSE, col.names = FALSE)
name <- paste("data_test/data_test_",type,"_n_",n,"_",i,".csv",sep="")
write.table(test, name, sep = ",", row.names = FALSE, col.names = FALSE)
}
#hist(y, breaks = 50, col = "lightblue", main = "Generated Mixture Data", freq = FALSE)
#lines(grid, true, col = "red", lwd = 2)
lines(test$Xtest,test$mXtest,col="red")
idx
t
